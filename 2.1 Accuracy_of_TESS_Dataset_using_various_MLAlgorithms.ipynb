{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "import soundfile # to read audio file\n",
    "import numpy as np #multi-dimensional arrays\n",
    "import librosa # to extract speech features\n",
    "import glob #to retrieve files/pathnames\n",
    "import os #provides functions for creating and removing a directory(folder)\n",
    "import pickle # to save model after training\n",
    "from sklearn.model_selection import train_test_split # for splitting training and testing\n",
    "from sklearn.metrics import accuracy_score # to measure how good we are"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.tree import DecisionTreeClassifier #Decision Tree Classifier\n",
    "from sklearn.neighbors import KNeighborsClassifier #K Nearest Neighbor Classifier\n",
    "import lightgbm as lgb #LGBM Classifier\n",
    "from sklearn.linear_model import LogisticRegression #Logistic Regression\n",
    "from sklearn.neural_network import MLPClassifier #Multi Layer Perceptron Classifier\n",
    "from sklearn.naive_bayes import GaussianNB #Gaussian Naive Bayes\n",
    "from sklearn.ensemble import RandomForestClassifier #Random Forest Classifier\n",
    "from sklearn.linear_model import SGDClassifier #Stochastic Gradient Descent Classifier\n",
    "from sklearn.svm import SVC #Support Vector Classifier\n",
    "import xgboost as xgb #eXtreme Gradient Boosting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "#extract features from soundfile\n",
    "def extract_feature(file_name, **kwargs):\n",
    "    mfcc = kwargs.get(\"mfcc\")\n",
    "    chroma = kwargs.get(\"chroma\")\n",
    "    mel = kwargs.get(\"mel\")\n",
    "    contrast = kwargs.get(\"contrast\")\n",
    "    tonnetz = kwargs.get(\"tonnetz\")\n",
    "    with soundfile.SoundFile(file_name) as sound_file:\n",
    "        A = sound_file.read(dtype=\"float32\")\n",
    "        sample_rate=sound_file.samplerate\n",
    "        if chroma:\n",
    "            stft=np.abs(librosa.stft(A))\n",
    "        result=np.array([])\n",
    "        if mfcc:\n",
    "            mfccs=np.mean(librosa.feature.mfcc(y=A, sr=sample_rate, n_mfcc=40).T, axis=0)\n",
    "        result=np.hstack((result, mfccs))\n",
    "        if chroma:\n",
    "            chroma=np.mean(librosa.feature.chroma_stft(S=stft, sr=sample_rate).T,axis=0)\n",
    "        result=np.hstack((result, chroma))\n",
    "        if mel:\n",
    "            mel=np.mean(librosa.feature.melspectrogram(A, sr=sample_rate).T,axis=0)\n",
    "        result=np.hstack((result, mel))\n",
    "        if contrast:\n",
    "            contrast=np.mean(librosa.feature.spectral_contrast(S=stft, sr=srate).T,axis=0)\n",
    "            result=np.hstack((result, contrast))\n",
    "        if tonnetz:\n",
    "            tonnetz=np.mean(librosa.feature.tonnetz(y=librosa.effects.harmonic(A), sr=srate).T,axis=0)\n",
    "            result=np.hstack((result, tonnetz))\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Emotions available in TESS Dataset\n",
    "emotions={\n",
    "  'neutral':'NEUTRAL',\n",
    "  'happy':'HAPPY',\n",
    "  'sad':'SAD',\n",
    "  'angry':'ANGRY',\n",
    "  'fear':'FEARFUL',\n",
    "  'disgust':'DISGUST',\n",
    "  'ps':'SURPRISE'\n",
    "}\n",
    "#Emotions we want to observe\n",
    "observed_emotions=['NEUTRAL', 'HAPPY', 'FEARFUL', 'DISGUST']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "#load the data\n",
    "def load_data(test_size=0.25):\n",
    "    a,b=[],[]\n",
    "    for file in glob.glob(\"E:\\Kaggle\\Toronto emotion speech set\\TESS\\Actor_*\\*.wav\"):\n",
    "        file_name=os.path.basename(file)\n",
    "        filename=file_name.split(\"_\")[2]\n",
    "        emotion=emotions[filename.split(\".wav\")[0]]\n",
    "        if emotion not in observed_emotions:\n",
    "            continue\n",
    "        feature=extract_feature(file, mfcc=True, chroma=True, mel=True)\n",
    "        a.append(feature)\n",
    "        b.append(emotion)\n",
    "    return train_test_split(np.array(a), b, test_size=test_size, random_state=9)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Split the dataset\n",
    "a_train,a_test,b_train,b_test=load_data(test_size=0.25)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1200, 400)\n",
      "Features extracted: 180\n"
     ]
    }
   ],
   "source": [
    "#Get the shape(number of elements) of the training and testing datasets\n",
    "print((a_train.shape[0], a_test.shape[0]))\n",
    "#Get the number of features extracted\n",
    "print(f'Features extracted: {a_train.shape[1]}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy using MLPClassifier: 100.00%\n"
     ]
    }
   ],
   "source": [
    "#MLPClassifier\n",
    "model_params = {\n",
    "    'alpha': 0.01,\n",
    "    'batch_size': 200,\n",
    "    'epsilon': 1e-08, \n",
    "    'hidden_layer_sizes': (600,), \n",
    "    'learning_rate': 'adaptive', \n",
    "    'max_iter': 500, \n",
    "}\n",
    "model = MLPClassifier(**model_params)\n",
    "#train the model\n",
    "model.fit(a_train, b_train)\n",
    "#predict values for test set\n",
    "b_pred=model.predict(a_test)\n",
    "#Calculate Accuracy\n",
    "accuracy = accuracy_score(y_true=b_test, y_pred=b_pred)\n",
    "print(\"Accuracy using MLPClassifier: {:.2f}%\".format(accuracy*100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Warning] min_data_in_leaf is set=27, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=27\n",
      "Accuracy using LGBMClassifier: 99.75%\n"
     ]
    }
   ],
   "source": [
    "#LGBMClassifier\n",
    "lgb_params = {'num_leaves': 22, \n",
    "              'max_depth': 37, \n",
    "              'n_estimators': 12310, \n",
    "              'subsample_for_bin': 491645, \n",
    "              'min_data_in_leaf': 27, \n",
    "              'reg_alpha': 1.744123586157066, \n",
    "              'colsample_bytree': 0.6495503686746514, \n",
    "              'learning_rate': 0.8581745963346554, \n",
    "              'boosting_type': 'dart'}\n",
    "model=lgb.LGBMClassifier(**lgb_params)\n",
    "#training\n",
    "model.fit(a_train, b_train)\n",
    "#predicting\n",
    "b_pred=model.predict(a_test)\n",
    "#accuracy\n",
    "accuracy = accuracy_score(y_true=b_test, y_pred=b_pred)\n",
    "print(\"Accuracy using LGBMClassifier: {:.2f}%\".format(accuracy*100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy using Random Forest: 100.00%\n"
     ]
    }
   ],
   "source": [
    "#Random Forest Classifier\n",
    "rf=RandomForestClassifier(criterion = 'entropy', \n",
    "             max_depth = 15, \n",
    "             n_estimators = 22984, \n",
    "             min_samples_leaf = 3, \n",
    "             min_samples_split = 9, \n",
    "             max_leaf_nodes = 239, \n",
    "             random_state = 22)\n",
    "rf.fit(a_train, b_train)\n",
    "b_pred=rf.predict(a_test)\n",
    "accuracy = accuracy_score(y_true=b_test, y_pred=b_pred)\n",
    "print(\"Accuracy using Random Forest: {:.2f}%\".format(accuracy*100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy using SVClassifier: 100.00%\n"
     ]
    }
   ],
   "source": [
    "#Support Vector Classifier\n",
    "svclassifier = SVC(kernel = 'linear')\n",
    "svclassifier.fit(a_train, b_train)\n",
    "y_preds = svclassifier.predict(a_test)\n",
    "accuracy = accuracy_score(y_true=b_test, y_pred=y_preds)\n",
    "print(\"Accuracy using SVClassifier: {:.2f}%\".format(accuracy*100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy using Logistic Regression: 100.00%\n"
     ]
    }
   ],
   "source": [
    "#LogisticRegression\n",
    "lr=LogisticRegression(multi_class='multinomial',\n",
    "             class_weight = None, \n",
    "             solver = 'saga', \n",
    "             max_iter = 10000)\n",
    "lr.fit(a_train,b_train)\n",
    "b_pred=lr.predict(a_test)\n",
    "accuracy = accuracy_score(y_true=b_test, y_pred=b_pred)\n",
    "print(\"Accuracy using Logistic Regression: {:.2f}%\".format(accuracy*100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy using K Nearest Neighbor: 99.00%\n"
     ]
    }
   ],
   "source": [
    "#K Nearest Neighbors\n",
    "knn = KNeighborsClassifier(weights='distance', n_neighbors=32)\n",
    "knn.fit(a_train, b_train)\n",
    "b_pred=knn.predict(a_test)\n",
    "accuracy = accuracy_score(y_true=b_test, y_pred=b_pred)\n",
    "print(\"Accuracy using K Nearest Neighbor: {:.2f}%\".format(accuracy*100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[13:43:09] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "Accuracy using XGBClassifier: 97.50%\n"
     ]
    }
   ],
   "source": [
    "#XGradientBoost\n",
    "xgb_params = {'booster': 'gbtree', \n",
    "              'lambda': 7.201651687969849e-08, \n",
    "              'alpha': 2.2495125443474775e-05, \n",
    "              'max_depth': 7, \n",
    "              'eta': 9.307925211476325e-06, \n",
    "              'gamma': 1.7948741419263195e-05, \n",
    "              'grow_policy': 'lossguide'}\n",
    "model=xgb.XGBClassifier(**xgb_params)\n",
    "model.fit(a_train, b_train)\n",
    "b_pred=model.predict(a_test)\n",
    "accuracy = accuracy_score(y_true=b_test, y_pred=b_pred)\n",
    "print(\"Accuracy using XGBClassifier: {:.2f}%\".format(accuracy*100))\n",
    "\n",
    "import warnings\n",
    "def fxn():\n",
    "    warnings.warn(\"deprecated\", DeprecationWarning)\n",
    "\n",
    "with warnings.catch_warnings():\n",
    "    warnings.simplefilter(\"ignore\")\n",
    "    fxn()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy using Decision Tree Classifier: 95.50%\n"
     ]
    }
   ],
   "source": [
    "#Decision Tree\n",
    "dtree = DecisionTreeClassifier(criterion = 'entropy', \n",
    "             max_depth = 35, \n",
    "             min_samples_leaf = 4, \n",
    "             min_samples_split = 23, \n",
    "             max_leaf_nodes = 169)\n",
    "dtree.fit(a_train, b_train)\n",
    "predictions = dtree.predict(a_test)\n",
    "accuracy = accuracy_score(y_true=b_test, y_pred=predictions)\n",
    "print(\"Accuracy using Decision Tree Classifier: {:.2f}%\".format(accuracy*100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy using Naive Bayes: 86.25%\n"
     ]
    }
   ],
   "source": [
    "#Naive Baye's\n",
    "nb=GaussianNB()\n",
    "nb.fit(a_train, b_train)\n",
    "b_pred=nb.predict(a_test)\n",
    "accuracy = accuracy_score(y_true=b_test, y_pred=b_pred)\n",
    "print(\"Accuracy using Naive Bayes: {:.2f}%\".format(accuracy*100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy using Stochastic Gradient Descent: 100.00%\n"
     ]
    }
   ],
   "source": [
    "#Stochastic Gradient Descent\n",
    "sgd=SGDClassifier(loss='modified_huber', shuffle=True, random_state=101)\n",
    "sgd.fit(a_train, b_train)\n",
    "b_pred=sgd.predict(a_test)\n",
    "accuracy = accuracy_score(y_true=b_test, y_pred=b_pred)\n",
    "print(\"Accuracy using Stochastic Gradient Descent: {:.2f}%\".format(accuracy*100))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
